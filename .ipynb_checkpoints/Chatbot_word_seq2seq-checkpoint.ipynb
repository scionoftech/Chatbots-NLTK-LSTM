{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXV6uvdo_OuD"
   },
   "source": [
    "# Chatbot using LSTM-Encoder-Decoder\n",
    "\n",
    "The Encoder-Decoder LSTM is a recurrent neural network designed to address sequence-to-sequence problems, sometimes called seq2seq.\n",
    "\n",
    "Sequence-to-sequence prediction problems are challenging because the number of items in the input and output sequences can vary. For example, text translation and learning to execute programs are examples of seq2seq problems.\n",
    "\n",
    "One approach to seq2seq prediction problems that has proven very effective is called the Encoder-Decoder LSTM.\n",
    "\n",
    "This architecture is comprised of two models: one for reading the input sequence and encoding it into a fixed-length vector, and a second for decoding the fixed-length vector and outputting the predicted sequence. The use of the models in concert gives the architecture its name of Encoder-Decoder LSTM designed specifically for seq2seq problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn.png](encoder_decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seq model will be trained on [Cornell Movie - Dialogs Corpus](https://www.kaggle.com/Cornell-University/movie-dialog-corpus) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZV1qiVFZ3FH"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTiLdF2MZ3FQ"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "HIDDEN_UNITS = 256\n",
    "MAX_INPUT_SEQ_LENGTH = 40\n",
    "MAX_TARGET_SEQ_LENGTH = 40\n",
    "MAX_VOCAB_SIZE = 800\n",
    "DATA_PATH = 'dataset/movie_lines_cleaned_10k.txt'\n",
    "WEIGHT_FILE_PATH = 'models/word-seq.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9wB69JQhZ3FT"
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "lines = open(DATA_PATH, 'rt', encoding='utf8').read().split('\\n')\n",
    "input_texts = []\n",
    "target_texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44cH4GetGSlk"
   },
   "source": [
    "## PART 1 - DATA PREPROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMrqjAO5GeFJ"
   },
   "outputs": [],
   "source": [
    "input_counter = Counter()\n",
    "target_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOPCvwjGZ3FZ"
   },
   "outputs": [],
   "source": [
    "# Getting separately the questions and the answers\n",
    "prev_words = []\n",
    "for line in lines:\n",
    "\n",
    "    next_words = [w.lower() for w in nltk.word_tokenize(line)]\n",
    "    if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n",
    "        next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n",
    "\n",
    "    if len(prev_words) > 0:\n",
    "        input_texts.append(prev_words)\n",
    "        for w in prev_words:\n",
    "            input_counter[w] += 1\n",
    "\n",
    "        target_words = next_words[:]\n",
    "        target_words.insert(0, 'START')\n",
    "        target_words.append('END')\n",
    "        for w in target_words:\n",
    "            target_counter[w] += 1\n",
    "        target_texts.append(target_words)\n",
    "\n",
    "    prev_words = next_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTgmxieXZ3Fd"
   },
   "outputs": [],
   "source": [
    "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
    "input_word2idx = dict()\n",
    "target_word2idx = dict()\n",
    "for idx, word in enumerate(input_counter.most_common(MAX_VOCAB_SIZE)):\n",
    "    input_word2idx[word[0]] = idx + 2\n",
    "for idx, word in enumerate(target_counter.most_common(MAX_VOCAB_SIZE)):\n",
    "    target_word2idx[word[0]] = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beyD6jiMZ3Fg"
   },
   "outputs": [],
   "source": [
    "# Adding the last tokens to these two dictionaries\n",
    "input_word2idx['PAD'] = 0\n",
    "input_word2idx['UNK'] = 1\n",
    "target_word2idx['UNK'] = 0\n",
    "\n",
    "# Creating the inverse dictionary of the answerswords2int dictionary\n",
    "input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "\n",
    "num_encoder_tokens = len(input_idx2word)\n",
    "num_decoder_tokens = len(target_idx2word)\n",
    "\n",
    "np.save(project_path+'models/word-input-word2idx.npy', input_word2idx)\n",
    "np.save(project_path+'models/word-input-idx2word.npy', input_idx2word)\n",
    "np.save(project_path+'models/word-target-word2idx.npy', target_word2idx)\n",
    "np.save(project_path+'models/word-target-idx2word.npy', target_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RGxcGbSoZ3Fk",
    "outputId": "9ccd88b9-8339-4ee9-f5d3-07cd13afa6f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_encoder_tokens': 802, 'num_decoder_tokens': 801, 'encoder_max_seq_length': 40, 'decoder_max_seq_length': 42}\n"
     ]
    }
   ],
   "source": [
    "# Translating all the questions and the answers into integers\n",
    "encoder_input_data = []\n",
    "\n",
    "encoder_max_seq_length = 0\n",
    "decoder_max_seq_length = 0\n",
    "\n",
    "for input_words, target_words in zip(input_texts, target_texts):\n",
    "    encoder_input_wids = []\n",
    "    for w in input_words:\n",
    "        w2idx = 1  # default [UNK]\n",
    "        if w in input_word2idx:\n",
    "            w2idx = input_word2idx[w]\n",
    "        encoder_input_wids.append(w2idx)\n",
    "\n",
    "    encoder_input_data.append(encoder_input_wids)\n",
    "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
    "    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)\n",
    "\n",
    "context = dict()\n",
    "context['num_encoder_tokens'] = num_encoder_tokens\n",
    "context['num_decoder_tokens'] = num_decoder_tokens\n",
    "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
    "context['decoder_max_seq_length'] = decoder_max_seq_length\n",
    "\n",
    "print(context)\n",
    "np.save(project_path+'models/word-context.npy', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSsw4I4LZ3Fn"
   },
   "outputs": [],
   "source": [
    "# generate traning and testing batches\n",
    "def generate_batch(input_data, output_text_data):\n",
    "    num_batches = len(input_data) // BATCH_SIZE\n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * BATCH_SIZE\n",
    "            end = (batchIdx + 1) * BATCH_SIZE\n",
    "            encoder_input_data_batch = pad_sequences(input_data[start:end], encoder_max_seq_length)\n",
    "            decoder_target_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
    "            decoder_input_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
    "            for lineIdx, target_words in enumerate(output_text_data[start:end]):\n",
    "                for idx, w in enumerate(target_words):\n",
    "                    w2idx = 0  # default [UNK]\n",
    "                    if w in target_word2idx:\n",
    "                        w2idx = target_word2idx[w]\n",
    "                    decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
    "                    if idx > 0:\n",
    "                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Y7P_CDrS3HX"
   },
   "source": [
    "## PART 2 - BUILDING THE SEQ2SEQ MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "lXJm3yzbZ3Fq",
    "outputId": "353bdb96-650d-4dc1-c871-2d24cac67d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3664"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the Encoder RNN\n",
    "encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=HIDDEN_UNITS,\n",
    "                              input_length=encoder_max_seq_length, name='encoder_embedding')\n",
    "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "# Creating the Decoder RNN\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "                                                                 initial_state=encoder_states)\n",
    "decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "json = model.to_json()\n",
    "open(project_path+'models/word-architecture.json', 'w').write(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cQbI9qQbZ3Ft",
    "outputId": "3a73fbad-4d95-47eb-b015-ae4217495597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# create train and test sets\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(encoder_input_data, target_texts, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(Xtrain))\n",
    "print(len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "daq05g_ZZ3Fw"
   },
   "outputs": [],
   "source": [
    "# train and test batch size \n",
    "train_num_batches = len(Xtrain) // BATCH_SIZE\n",
    "test_num_batches = len(Xtest) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCNwsHvvZ3Fy"
   },
   "outputs": [],
   "source": [
    "# generate train and test datra\n",
    "train_gen = generate_batch(Xtrain, Ytrain)\n",
    "test_gen = generate_batch(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbPsoqDEZ3F2"
   },
   "outputs": [],
   "source": [
    "# callbacks\n",
    "# checkpoint = ModelCheckpoint(WEIGHT_FILE_PATH, monitor ='val_loss', verbose = 1, save_best_only = True, mode ='max')\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cj-aSYheVaQh"
   },
   "source": [
    "## PART 3 - TRAINING THE SEQ2SEQ MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "O5u-p955Z3F5",
    "outputId": "489effaa-e83c-4006-a886-5ed9909eb080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "124/124 [==============================] - 18s 145ms/step - loss: 1.5133 - val_loss: 1.4335\n",
      "\n",
      "Epoch 00001: val_loss improved from -inf to 1.43352, saving model to /content/drive/My Drive/DLCP/openwork/Chatbot_with_Encoder_Decoder/models/word-weights.h5\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.4273 - val_loss: 1.3501\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.43352\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.3438 - val_loss: 1.2807\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.43352\n",
      "Epoch 4/100\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.2803 - val_loss: 1.2325\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.43352\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.2349 - val_loss: 1.2041\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.43352\n",
      "Epoch 6/100\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.2091 - val_loss: 1.1783\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.43352\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.1788 - val_loss: 1.1637\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.43352\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.1577 - val_loss: 1.1502\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.43352\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - 16s 131ms/step - loss: 1.1401 - val_loss: 1.1402\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.43352\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.1249 - val_loss: 1.1325\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.43352\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.1113 - val_loss: 1.1264\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.43352\n",
      "Epoch 12/100\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.0976 - val_loss: 1.1212\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.43352\n",
      "Epoch 13/100\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.0853 - val_loss: 1.1175\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.43352\n",
      "Epoch 14/100\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.0731 - val_loss: 1.1172\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.43352\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.0615 - val_loss: 1.1151\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.43352\n",
      "Epoch 16/100\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.0501 - val_loss: 1.1144\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.43352\n",
      "Epoch 17/100\n",
      "124/124 [==============================] - 16s 129ms/step - loss: 1.0385 - val_loss: 1.1152\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.43352\n",
      "Epoch 18/100\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.0271 - val_loss: 1.1162\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.43352\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb2015eef0>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    verbose=1, validation_data=test_gen, validation_steps=test_num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XU8deUYKZ3F9"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save_weights(WEIGHT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jg2D9ttbWrST"
   },
   "source": [
    "## PART 4 - TESTING THE SEQ2SEQ MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hN3eZtr6m4P3"
   },
   "outputs": [],
   "source": [
    "class ChatBot():\n",
    "    \"\"\"\n",
    "    This is ChatBot class it takes weights for the Neural Network, compliling model\n",
    "    and returns prediction in responce to input text\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        define all required parameters, rebuild model and load weights\n",
    "        \"\"\"\n",
    "        self.input_word2idx = np.load(project_path+'models/word-input-word2idx.npy',allow_pickle=True).item()\n",
    "        self.input_idx2word = np.load(project_path+'models/word-input-idx2word.npy',allow_pickle=True).item()\n",
    "        self.target_word2idx = np.load(project_path+'models/word-target-word2idx.npy',allow_pickle=True).item()\n",
    "        self.target_idx2word = np.load(project_path+'models/word-target-idx2word.npy',allow_pickle=True).item()\n",
    "        context = np.load(project_path+'models/word-context.npy',allow_pickle=True).item()\n",
    "        self.max_encoder_seq_length = context['encoder_max_seq_length']\n",
    "        self.max_decoder_seq_length = context['decoder_max_seq_length']\n",
    "        self.num_encoder_tokens = context['num_encoder_tokens']\n",
    "        self.num_decoder_tokens = context['num_decoder_tokens']\n",
    "        self.ultimate_question = 'Answer to the Ultimate Question of Life, the Universe, and Everything'\n",
    "\n",
    "        encoder_inputs = Input(shape=(None, ), name='encoder_inputs')\n",
    "        encoder_embedding = Embedding(input_dim=self.num_encoder_tokens, output_dim=HIDDEN_UNITS,\n",
    "                                      input_length=self.max_encoder_seq_length, name='encoder_embedding')\n",
    "        encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name=\"encoder_lstm\")\n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "        encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "        decoder_inputs = Input(shape=(None, self.num_decoder_tokens), name='decoder_inputs')\n",
    "        decoder_lstm = LSTM(units=HIDDEN_UNITS, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        self.model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        # Loading the weights\n",
    "        self.model.load_weights(project_path+'models/word-seq.h5')\n",
    "        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "        self.encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "        decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "    # Setting up the chat\n",
    "    def reply(self, input_text):\n",
    "        \"\"\"\n",
    "        Takes input_text and return predicted responce\n",
    "        :param input_text: string\n",
    "        :return: predicted_text: string\n",
    "        \"\"\"\n",
    "        if input_text == self.ultimate_question:\n",
    "            return '42'\n",
    "        input_seq = []\n",
    "        input_wids = []\n",
    "        for word in nltk.word_tokenize(input_text.lower()):\n",
    "            idx = 1\n",
    "            if word in self.input_word2idx:\n",
    "                idx = self.input_word2idx[word]\n",
    "            input_wids.append(idx)\n",
    "        input_seq.append(input_wids)\n",
    "        input_seq = pad_sequences(input_seq, self.max_encoder_seq_length)\n",
    "        states_value = self.encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "        target_seq[0, 0, self.target_word2idx['START']] = 1\n",
    "        target_text = ''\n",
    "        target_text_len = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
    "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "            sample_word = self.target_idx2word[sample_token_idx]\n",
    "            target_text_len += 1\n",
    "\n",
    "            if sample_word != 'START' and sample_word != 'END':\n",
    "                target_text += ' ' + sample_word\n",
    "\n",
    "            if sample_word == 'END' or target_text_len >= self.max_decoder_seq_length:\n",
    "                terminated = True\n",
    "\n",
    "            target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "            target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "            states_value = [h, c]\n",
    "        return target_text.strip().replace('UNK', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8REpl6vSuJyy"
   },
   "outputs": [],
   "source": [
    "# start chatbot\n",
    "bot = ChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i4qO3PnmuNiu",
    "outputId": "d0247265-bd5f-4d28-854f-1ffe803929c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i do n't know .\""
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.reply('Where are you from?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tMeySSApul-7",
    "outputId": "0f2e49a1-e788-4168-f275-6859423a2eca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ,  .'"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.reply('What is your name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DW7LeFkUuxam"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chatbot_word_seq2seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
